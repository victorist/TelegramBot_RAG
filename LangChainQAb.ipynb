{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7f97e0",
   "metadata": {},
   "source": [
    "## Строим вопрос-ответного бота по технологии Retrieval-Augmented Generation на LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com) - это набирающая популярность библиотека для работы с большими языковыми моделями и для построения конвейеров обработки текстовых данных. В одной библиотеке присутствуют все элементы, которые помогут нам создать вопрос-ответного бота на наборе текстовых данных: вычисление эмбеддингов, запуск больших языковых моделей для генерации текста, поиск по ключу в векторных базах данных и т.д.\n",
    "\n",
    "Для начала, установим `langchain` и сопутствующие библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "\n",
    "%pip install -q langchain sentence_transformers lancedb unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eab2d4",
   "metadata": {},
   "source": [
    "## Загружаем документы и разбиваем на части\n",
    "\n",
    "Для загрузки документов используется `DirectoryLoader`, который загружает файлы из указанной диреткории. Поддерживаются загрузка различных типов файлов - документацию по загрузчику смотри [здесь](https://langchain-fanyi.readthedocs.io/en/latest/modules/indexes/document_loaders/examples/directory_loader.html). \n",
    "\n",
    "Можно использовать загрузчик *файлов* `UnstructuredFileLoader` (по-умолчанию, `DirectoryLoader` использует этот метод), в параметрах которого можно задать точноcть разбиения документов. Загрузчик неструктурированных документов позволяет пользователям передавать strategy параметр, который позволяет unstructured узнать, как разбить документ на разделы. В настоящее время поддерживаются следующие стратегии: \"hi_res\" (по умолчанию) и \"fast\". Стратегии разбиения на разделы в высоком разрешении более точны, но требуют больше времени для обработки. Быстрые стратегии позволяют быстрее разбивать документ на разделы, но при этом снижают точность. Не для всех типов документов существуют отдельные стратегии разбиения в высоком разрешении и быстром режиме. Для этих типов документов strategy параметр kwarg игнорируется. В некоторых случаях стратегия высокого разрешения будет заменена на fast, если отсутствует зависимость (например, модель разделения документа).\n",
    "\n",
    "**Unstructured** создает разные “элементы” для разных фрагментов текста. По умолчанию мы объединяем их вместе, но вы можете легко сохранить это разделение, указав mode=\"elements\".\n",
    "\n",
    "В настоящее время **Unstructured** поддерживает загрузку текстовых файлов, Powerpoint, HTML, PDF-файлов, изображений и многого другого.\n",
    "\n",
    "Для работы **retrival augmented generation** нам необходимо по запросу найти наиболее релевантные фрагменты исходного текста, на основе которых будет формироваться ответ. Для этого нам надо разбить текст на такие фрагменты, по которым мы сможем вычислять эмбеддинг, и которые будут с запасом помещаться во входное окно используемой большой языковой модели.\n",
    "\n",
    "Для этого можно использовать механизмы фреймворка **LangChain** - например, `RecursiveCharacterTextSplitter`. Он разбивает текст на перекрывающиеся фрагменты по набору типовых разделителей - абзацы, переводы строк, разделители слов.\n",
    "\n",
    "> **ВАЖНО**: Перед выполнением ячейки не забудьте установить имя пользователя, которое вы использовали на предыдущем шаге.\n",
    "\n",
    "Размер `chunk_size` нужно выбирать исходя из нескольких показателей:\n",
    "* Допустимая длина контекста для эмбеддинг-модели. Yandex GPT Embeddings допускают 2048 токенов, в то время как многие открытые модели HuggingFace имеют длину контекста 512-1024 токена\n",
    "* Допустимый размер окна контекста большой языковой модели. Если мы хотим использовать в запросе top 3 результатов поиска, то 3 * chunk_size+prompt_size+response_size должно не превышать длины контекста модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "\n",
    "import langchain\n",
    "import langchain.document_loaders\n",
    "# про сплиттер\n",
    "# https://python.langchain.com/docs/integrations/vectorstores/vald#basic-example\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c154f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "\n",
    "#user = 'shwars'\n",
    "chunk_size = 1024   # 2048\n",
    "chunk_overlap= 128  #50\n",
    "#source_dir = f\"/home/jupyter/datasphere/s3/s3store/{user}/text\"\n",
    "source_dir = 'txts'\n",
    "\n",
    "loader = langchain.document_loaders.DirectoryLoader(source_dir,glob=\"*.txt\",show_progress=True,recursive=True)\n",
    "splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "fragments = splitter.create_documents([ x.page_content for x in loader.load()])\n",
    "print('Кол-во фрагментов текстов:',len(fragments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187a7f0",
   "metadata": {},
   "source": [
    "## Вычисляем эмбеддинги для всех фрагментов\n",
    "\n",
    "Для вычисления эмбеддингов можно взять какую-нибудь модель из репозитория `HuggingFace`, с поддержкой русского языка. `LangChain` содержит свои встроенные средства работы с эмбеддингами, и поддерживает модели из `HuggingFace`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЭТО ПРИМЕР с сайта \n",
    "# https://python.langchain.com/docs/integrations/vectorstores/vald#basic-example\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Vald\n",
    "\n",
    "raw_documents = TextLoader(\"state_of_the_union.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = Vald.from_documents(documents, embeddings, host=\"localhost\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "# Этотак просто, для проверки возможностей эмбединга \n",
    "\n",
    "embeddings = langchain.embeddings.HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v1\")\n",
    "sample_vec = embeddings.embed_query(\"Hello, world!\")\n",
    "len(sample_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed51ad6",
   "metadata": {},
   "source": [
    "Можно использовать более продвинутую модель [эмбеддингов от Yandex GPT](https://cloud.yandex.ru/docs/yandexgpt/api-ref/Embeddings/). Вот так можно вызывать её через API Yandex Cloud. Не забудьте при необходимости исправить параметры `api_key` и `folder_id` в соответствии с вашими данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "# Лучше использовать json-конфиг\n",
    "import os\n",
    "#api_key = os.environ['api_key']\n",
    "api_key = \"\"\n",
    "folder_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c26231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# НЕ ИСПОЛЬЗУЕМ __________________________________________________________________________\n",
    "import requests\n",
    "\n",
    "headers={ \n",
    "    \"Authorization\" : f\"Api-key {api_key}\",\n",
    "    \"x-folder-id\" : folder_id\n",
    "}\n",
    "j = {\n",
    "  \"model\" : \"general:embedding\",\n",
    "  \"embedding_type\" : \"EMBEDDING_TYPE_DOCUMENT\",\n",
    "  \"text\": \"Hello, world!\"\n",
    "}\n",
    "\n",
    "res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/embedding\",json=j,headers=headers)\n",
    "vec = res.json()['embedding']\n",
    "print(\"Длина вектора\",len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17471c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yandex-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПРИМЕР ИЗ ДОКУМЕНТАЦИИ  ##############################\n",
    "\n",
    "from yandex_chain import YandexLLM, YandexEmbeddings\n",
    "#from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "docs = \"How are you today?\"\n",
    "embeddings = YandexEmbeddings(config=\"config.json\")\n",
    "vectorstore = FAISS.from_texts(docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40a5ec",
   "metadata": {},
   "source": [
    "## Cохраняем эмбеддинги  в векторную БД\n",
    "\n",
    "Для поиска нам нужно уметь быстро сопоставлять эмбеддинг запроса, и эмбеддинги всех фрагементов наших исходных материалов. Для этого используются векторные базы данных. Для крупных проектов имеет смысл использовать серьезные инструменты, типа [OpenSearch](https://opensearch.org/) (доступный [в виде сервиса в облаке Yandex Cloud](https://cloud.yandex.ru/services/managed-opensearch)), но для нашего примера мы используем небольшую векторную БД [LanceDB](https://lancedb.com/), хранящую индекс в директории на диске. (Примеры использования различных БД можно посмотреть [здесь](https://python.langchain.com/docs/integrations/vectorstores))\n",
    "\n",
    "Первоначально создадим базу данных, добавив туда одну строчку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok ### ВКЛЮЧАЕМ В КОД\n",
    "# Первоначально создадим базу данных, добавив туда одну строчку\n",
    "\n",
    "#from langchain.vectorstores import LanceDB\n",
    "import lancedb\n",
    "import os\n",
    "#import tqdm as notebook_tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from yandex_chain import YandexLLM, YandexEmbeddings\n",
    "\n",
    "\n",
    "embeddings = YandexEmbeddings(config=\"config.json\")\n",
    "\n",
    "db_dir = \"store\"\n",
    "\n",
    "os.makedirs(db_dir,exist_ok=True)\n",
    "\n",
    "db = lancedb.connect(db_dir)\n",
    "\n",
    "table = db.create_table(\n",
    "    \"vector_index\",\n",
    "    data=[\n",
    "        {\n",
    "            \"vector\": embeddings.embed_query(\"Hello World\"),\n",
    "            \"text\": \"Hello World\",\n",
    "            \"id\": \"1\",\n",
    "        }\n",
    "    ],\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5050e1",
   "metadata": {},
   "source": [
    "Далее проиндексируем все выделенные ранее фрагменты нашей документации. \n",
    "\n",
    "> В зависимости от объема текста, эта ячейка может выполняться достаточно длительное время - вспомним про задержку в 1 сек между запросами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok ### ВКЛЮЧАЕМ С КОД\n",
    "\n",
    "db = LanceDB.from_documents(fragments, embeddings, connection=table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# требуется для выполнения запрос-ответ\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a5c77",
   "metadata": {},
   "source": [
    "Теперь посмотрим, насколько хорошо находятся фрагменты текста по какому-то запросу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfee6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "# ВКЛЮЧАЕМ В КОД СТОРОЙ СПОСОБ - ячейка ниже\n",
    "\n",
    "q=\"Можно ли провозить лыжи на борту самолёта?\"\n",
    "# q=\"Я инвалид-колясочник. Что мне нужно сделать, чтобы мне помогли в аэропорту?\"\n",
    "#q=\"Спасибо\"\n",
    "\n",
    "\n",
    "res = db.similarity_search(q)\n",
    "for x in res:\n",
    "    print('-'*50)\n",
    "    print(x.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2570083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbb2c1",
   "metadata": {},
   "source": [
    "Ещё один полезный интерфейс для поиска текстов - это `Retriever`, убедимся, что он тоже работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok ВКЛЮЧАЕМ В КОД #################################\n",
    "\n",
    "#q=\"Можно ли провозить лыжи на борту самолёта?\"\n",
    "q=\"Я инвалид-колясочник. Что мне нужно сделать, чтобы мне помогли в аэропорту?\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "res = retriever.get_relevant_documents(q)\n",
    "\n",
    "concatdocs = '\\n'\n",
    "for x in res:\n",
    "    print(x.page_content)\n",
    "    print('________')\n",
    "#    concatdocs += x.page_content\n",
    "\n",
    "#print(concatdocs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe8e23",
   "metadata": {},
   "source": [
    "## Примечание\n",
    "### Подбор `search_kwargs`\n",
    "Опыт показал, что требуется подбор `search_kwargs`. Для текущей задачи при k=5 бот не даёт ответ на вопрос __Я инвалид-колясочник. Что мне нужно сделать, чтобы мне помогли в аэропорту?__. \n",
    "\n",
    "Ответ получается такой: __К сожалению, я не могу ничего сказать об этом. Давайте сменим тему?__. \n",
    "\n",
    "### Хороший ответ получается при k=15...20.\n",
    "Пример ответа пр  k=15 представлен ниже. Важно отметить, что при увеличении `k` может ворзникнуть ошибка из-за ограниченний по кол-ву токенов, обрабатываемых моделью.\n",
    "\n",
    "```md\n",
    "Согласно правилам перевозки авиакомпании «АЗУР эйр», если вы заранее согласовали данную услугу с авиакомпанией, а ваш багаж прошёл специальный досмотр на авиационную безопасность, то вам должны помочь в аэропорту.\n",
    "\n",
    "Вы можете согласовать перевозку вашего кресла-коляски как сверхнормативного багажа, если это было заранее согласовано с авиакомпанией и оплачено.\n",
    "\n",
    "Если у вас есть вопросы, не освещённые в данных правилах, вы можете позвонить по телефону *7 (495) 374-55-14* или отправить запрос на электронную почту `call@azurair.ru`.\n",
    "\n",
    "Также авиакомпания может предоставить вам информацию о правилах перевозки, в том числе о перевозке кресел-колясок и других вспомогательных устройств передвижения, используемых пассажирами из числа инвалидов и других лиц с ограничениями жизнедеятельности.\n",
    "\n",
    "Пожалуйста, обратите внимание, что информация, содержащаяся в данном ответе, актуальна на момент публикации. Актуальную информацию вы можете найти на официальном сайте авиакомпании «АЗУР эйр».\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa9bc1",
   "metadata": {},
   "source": [
    "## Подключаемся к Yandex GPT\n",
    "\n",
    "Фреймворк LangChain поддерживает интеграцию с различными большими языковыми моделями, но Yandex GPT в их число не входит. Поэтому, как и в случае с эмбеддингами, нам надо написать соответствующий адаптер, предоставляющий доступ к Yandex GPT в формате LangChain. Для подробностей вызова Yandex GPT можно обратиться к документации по [YandexGPT API](https://cloud.yandex.ru/docs/yandexgpt/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f47730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ЭТО не включаем в код ________________________________________\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "import requests\n",
    "\n",
    "class YandexLLM(langchain.llms.base.LLM):\n",
    "    api_key: str = None\n",
    "    iam_token: str = None\n",
    "    folder_id: str\n",
    "    max_tokens : int = 1500\n",
    "    temperature : float = 1\n",
    "    instruction_text : str = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"yagpt\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        headers = { \"x-folder-id\" : self.folder_id }\n",
    "        if self.iam_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.iam_token}\"\n",
    "        if self.api_key:\n",
    "            headers[\"Authorization\"] = f\"Api-key {self.api_key}\"\n",
    "        req = {\n",
    "          \"model\": \"general\",\n",
    "          \"instruction_text\": self.instruction_text,\n",
    "          \"request_text\": prompt,\n",
    "          \"generation_options\": {\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature\n",
    "          }\n",
    "        }\n",
    "        res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/instruct\",\n",
    "          headers=headers, json=req).json()\n",
    "        return res['result']['alternatives'][0]['text']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"max_tokens\": self.max_tokens, \"temperature\" : self.temperature }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e363d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok #################################\n",
    "# ВКЛЮЧАЕМ В КОД импорт библиотек (скорее всего достаточно импортировтаь from langchain.prompts import ChatPromptTemplate )\n",
    "# КОД ИЗ ДОКУМЕНТАЦИИ yandex-chain\n",
    "# НЕ СКЛЮЧАеМ С КОД ________________________________________\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "'''\n",
    "# Отвечай на вопросы только в контексте следующих документв:\n",
    "#  Answer the question based only on the following context:\n",
    "template = \"\"\" Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = YandexLLM(config=\"config_neurocat.json\", use_lite=False)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# код мз документации НЕ СРАБОТАЛ\n",
    "#query = 'Можно ли провозить аккумуляторы в салоне самолёта?'\n",
    "chain.invoke(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd0582",
   "metadata": {},
   "source": [
    "Теперь попросим модель ответить на наш вопрос от лица сотрудника компании:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok - включаем в код #################################\n",
    "\n",
    "instructions = \"\"\"\n",
    "Ты информационный бот авиакомпании \"АЗУР эйр\" по имени Роберт. \n",
    "Твоя задача - полно и вежливо отвечать на вопросы собеседника.\n",
    "\"\"\"\n",
    "\n",
    "llm = YandexLLM(config=\"config.json\", use_lite=False, temperature= 0.2,         #  api_key=api_key, folder_id=folder_id,\n",
    "                instruction_text = instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dff326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok  не нужно включать в код ________________________________________\n",
    "# Это просто проверка работы модели без привязки к контенту документов\n",
    "q = 'Можно ли провозить аккумуляторы в салоне самолёта?'\n",
    "print('Вопрос клиента:', q)\n",
    "print('Ответ АзурБота:\\n',llm(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8c46e",
   "metadata": {},
   "source": [
    "В данном примере мы пока что никак не использовали наши текстовые документы.\n",
    "\n",
    "## Собираем Retrieval-Augmented Generation\n",
    "\n",
    "Пришла пора соединить всё вместе и научить бота отвечать на вопросы, подглядывая в наш текстовый корпус. Для этого используем механизм цепочек (*chains*). Основную функциональность реализует `StuffDocumentsChain`, которая делает следующее:\n",
    "\n",
    "1. Берёт коллекцию документов `input_documents`\n",
    "1. Каждый из них пропускает через `document_prompt`, и затем объединяет вместе.\n",
    "1. Данный текст помещается в переменную `document_variable_name`, и передаётся большой языковой модели `llm`\n",
    "\n",
    "В нашем случае `document_prompt` не будет модицицировать документ, а основной запрос к LLM будет содержать в себе инструкции для Yandex GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d75e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Промпт для обработки документов ### ВКЛЮЧАЕМ В КОД\n",
    "# Ответ на вопрос выдаётся в контексте релевантных документов (fragments), сохранённых в res\n",
    "from langchain import chains\n",
    "\n",
    "document_prompt = langchain.prompts.PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    "    )\n",
    "\n",
    "# Промпт для языковой модели\n",
    "document_variable_name = \"context\"\n",
    "'''\n",
    "stuff_prompt_override = \"\"\"\n",
    "Пожалуйста, отвечай на вопрос только в контексте правил воздушных перевозок авиакомпании \"АЗУР эйр\".\n",
    "Если ответа в правилах нет, извениьсь и порекомендуй позвонить в контактный ценр по телефону +7(495)374-55-14 или отправить запрос на электронную почту `call@azurair.ru`.\n",
    "Если собеседник не задал вопрос, поприветствуй его и сообщи о своей готворности ответить на его вопросы.\n",
    "Вот правила воздушных перевозок:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Вопрос:\n",
    "{query}\"\"\"\n",
    "'''\n",
    "stuff_prompt_override = \"\"\"\n",
    "Отвечай на вопрос только в контексте правил воздушных перевозок авиакомпании \"АЗУР эйр\"\n",
    "Если в правилах нет ответа на вопрос собеседника, рекомендуй позвонить в контактный ценр по телефону +7(495)374-55-14 или отправить запрос на электронную почту `call@azurair.ru\n",
    "При ответе учитывай нижеследующие выдержки из правил воздушных перевозок авиакомпании \"АЗУР эйр\":\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Вопрос:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = langchain.prompts.PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    "    )\n",
    "\n",
    "# Создаём цепочку\n",
    "llm_chain = langchain.chains.LLMChain(llm=llm, prompt=prompt)\n",
    "chain = langchain.chains.StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    "    )\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Запуск цепочки\n",
    "results = chain.run(input_documents=res, query=q)\n",
    "\n",
    "# Создание пути к файлу\n",
    "path = Path(\"answer.md\")\n",
    "\n",
    "# Запись результата в файл\n",
    "with open(path, \"w\") as file:\n",
    "    file.write(results)\n",
    "\n",
    "# Сообщение о завершении\n",
    "print(f\"Результат записан в файл: {path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd64df0",
   "metadata": {},
   "source": [
    "Чтобы ещё более улучшить результат, мы можем использовать хитрый механизм перемешивания документов, таким образом, чтобы наиболее значимые документы были ближе к началу запроса. Также мы оформим все операции в одну функцию `answer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK ==== Сортировка по релевантрости работает\n",
    "# Разобраться, куда код вставить\n",
    "\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "#reorderer = LongContextReorder(temperature=0.05, device='cuda', return_full_response=True)\n",
    "reorderer = LongContextReorder()\n",
    "\n",
    "# \"Температура\" модели. Чем выше значение, тем более \"творческими\" будут результаты. По умолчанию: 1.0\n",
    "# return_full_response (bool, optional) Если True, функция будет возвращать полный ответ модели, включая logits, probabilities и hidden states. По умолчанию: False\n",
    "\n",
    "def answer(query,reorder=True,print_results=False): # если reordr=True, то производится сортировка по степени релевантности\n",
    "  results = retriever.get_relevant_documents(query)\n",
    "  if print_results:\n",
    "        for x in results:\n",
    "            print(f\"{x.page_content}\\n--------\")\n",
    "  if reorder:\n",
    "    results = reorderer.transform_documents(results)\n",
    "  return chain.run(input_documents=results, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74834bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = answer(q, reorder=True, print_results=True)\n",
    "\n",
    "# Создание пути к файлу\n",
    "path = Path(\"answer_v2.md\")\n",
    "\n",
    "# Запись результата в файл\n",
    "with open(path, \"w\") as file:\n",
    "    file.write(result2)\n",
    "\n",
    "# Сообщение о завершении\n",
    "print(f\"Результат записан в файл: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8076eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример от Gemini ### НЕ ВКЛЮЧАЕМ В КОД\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "\n",
    "model = LongContextReorder()\n",
    "\n",
    "response = model(\"This is an example sentence.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb552db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример от Gemini ### НЕ ВКЛЮЧАЕМ В КОД\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "chain = (\n",
    "    chain.map(LongContextReorder())\n",
    "    .map(lambda x: x.text)\n",
    ")\n",
    "\n",
    "response = chain(\"This is an example sentence.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6c775",
   "metadata": {},
   "source": [
    "Можно сравнить результаты, выдаваемые Yandex GPT напрямую, с результатами нашей генерации на основе документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### НЕ ВКЛЮЧАЕМ В КОД\n",
    "\n",
    "def compare(q):\n",
    "    print(f\"Ответ YaGPT: {llm(q)}\")\n",
    "    print(f\"Ответ бота: {answer(q)}\")\n",
    "    \n",
    "compare(\"Что ты можешь сказать правилах перевозки лыж в самолёте?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb72f84",
   "metadata": {},
   "source": [
    "## Сохраняем векторную БД в Storage\n",
    "\n",
    "Для следующего этапа - вопрос-ответного бота - нам потребуется созданная нами база данных с документами. Поэтому скопируем её на хранилище s3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f31af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R ./store /home/jupyter/datasphere/s3/s3store/shwars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f29c666",
   "metadata": {},
   "source": [
    "# Создаём вопрос-ответного бота в телеграм\n",
    "Для создания вопрос-ответного бота нам потребуется развернуть нашу цепочку LangChain в виде публично-доступного веб-сервиса по HTTPS. Это удобнее всего сделать с помощью виртуальной машины Yandex Compute. Для понимания того, как устроены боты в телеграм, можно порекомендовать эту документацию.\n",
    "\n",
    "1. Создаём виртуальную машину. Для экспериментов нам не нужна высокая производительность, будет достаточно 4-6 Gb RAM, 50 Gb SSD, Ubuntu. Для входа на виртуальную машину используется ssh-сертификат.\n",
    "\n",
    " > Код телеграм-бота подразумевает, что пользователь на виртуальной машине будем иметь имя vmuser. Если вы используете другое имя, то придётся внести исправления в код.\n",
    "\n",
    "2. Создаём для виртуальной машины статический IP-адрес\n",
    "3. Для работы с телеграм потребуется HTTPS-протокол и сертификат SSL. Поэтому необходимо привязать к виртуальной машине какое-то доменное имя.\n",
    "4. Заходим в консоль виртуальной машины по SSH\n",
    "5. Клонируем репозиторий проекта `git clone https://github.com/yandex-datasphere/VideoQABot`\n",
    "6. Переходим в каталог проекта и устанавливаем зависимости:\n",
    "\n",
    "```bash\n",
    "cd VideoQABot\n",
    "pip3 install -r requirements.txt\n",
    "pip install waitress\n",
    "```\n",
    "\n",
    "```bash\n",
    "Installing collected packages: waitress\n",
    "  WARNING: The script waitress-serve is installed in '/home/azurbot/.local/bin' which is not on PATH.\n",
    "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "```\n",
    "\n",
    "\n",
    "7. Создаём SSL-сертификат для выбранного ранее доменного имени, это можно сделать, например, с помощью бесплатного сервиса Let's Encrypt и `certbot`\n",
    "8. Сертификаты записываем в директорию cert, и прописываем путь к ним в файле `telegram.py`\n",
    "9. Создаём бота в телеграм при помощи botfather (см. [докeнтацию](https://core.telegram.org/bots/tutorial#getting-ready)), и полученный telegram token записываем в `config.json`\n",
    "10. Также в `config.json` прописываем адрес нашего сайта. Рекомендуется использовать порт `8443`, поскольку в этом случае запускать веб-сервер можно от имени обычного пользователя.\n",
    "11. Копируем векторную базу данных, полученную на предыдущем шаге, в директорию store.\n",
    "12. Запускаем `python3 telegram.py`\n",
    "\n",
    "На этом этапе вы должны быть в состоянии послать в бота сообщения, текстом или как голосовое сообщение, и получить ответ, текстом + голосом."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
